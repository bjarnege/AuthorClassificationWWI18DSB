{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from RequestMapper import RequestMapper\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = pd.read_pickle(\"./Pipelines/ModelPipelines.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Age-Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_age = pipelines.values[0]\n",
    "\n",
    "X_test = pipeline_age.transformation.X_test\n",
    "X_test_numerical_transformed = pipeline_age.transformation.X_test_numerical_transformed\n",
    "X_test_text_transformed = pipeline_age.transformation.X_test_text_transformed\n",
    "y_test = pipeline_age.transformation.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_num = pipeline_age.modelling.numerical_model.predict(X_test_numerical_transformed)\n",
    "y_pred_text = pipeline_age.modelling.text_model.predict(X_test_text_transformed)\n",
    "y_pred_stacking = pipeline_age.modelling.weighted_prediction(X_test, None, \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric(y_pred_type, y_pred, y_test):\n",
    "    print(f\"Report for {y_pred_type} Dataset \\n\\n\")\n",
    "    print(\"Correlation matrix:\")\n",
    "    df_numerical_eval = pd.DataFrame({f\"{y_pred_type} predictions\":y_pred, \"Actual y\":y_test})\n",
    "    print(df_numerical_eval.corr())\n",
    "    print(f\"\\n\\nMean absolute error: {mean_absolute_error(y_test, y_pred)}\")\n",
    "    print(f\"R2-Score: {r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Numerical Dataset \n",
      "\n",
      "\n",
      "Correlation matrix:\n",
      "                       Numerical predictions  Actual y\n",
      "Numerical predictions               1.000000  0.644813\n",
      "Actual y                            0.644813  1.000000\n",
      "\n",
      "\n",
      "Mean absolute error: 4.48505021273046\n",
      "R2-Score: 0.3944441505103057\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Numerical\", y_pred_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Text Dataset \n",
      "\n",
      "\n",
      "Correlation matrix:\n",
      "                  Text predictions  Actual y\n",
      "Text predictions          1.000000  0.616172\n",
      "Actual y                  0.616172  1.000000\n",
      "\n",
      "\n",
      "Mean absolute error: 4.64777559344527\n",
      "R2-Score: 0.37890069088790757\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Text\", y_pred_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for stacking Dataset \n",
      "\n",
      "\n",
      "Correlation matrix:\n",
      "                      stacking predictions  Actual y\n",
      "stacking predictions              1.000000  0.727285\n",
      "Actual y                          0.727285  1.000000\n",
      "\n",
      "\n",
      "Mean absolute error: 4.046404894183239\n",
      "R2-Score: 0.5184081231923705\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"stacking\", y_pred_stacking, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Gender-Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gender = pipelines.values[1]\n",
    "\n",
    "X_test = pipeline_gender.transformation.X_test\n",
    "X_test_numerical_transformed = pipeline_gender.transformation.X_test_numerical_transformed\n",
    "X_test_text_transformed = pipeline_gender.transformation.X_test_text_transformed\n",
    "y_test = pipeline_gender.transformation.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_num = pipeline_gender.modelling.numerical_model.predict(X_test_numerical_transformed)\n",
    "y_pred_text = pipeline_gender.modelling.text_model.predict(X_test_text_transformed)\n",
    "y_pred_stacking = pipeline_gender.modelling.weighted_prediction(X_test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric(y_pred_type, y_pred, y_test):\n",
    "    print(f\"Report for {y_pred_type} Dataset \\n\\n\")\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f\"\\n\\nAccuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Numerical Dataset \n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[47019 12481]\n",
      " [14954 46036]]\n",
      "\n",
      "\n",
      "Accuracy: 0.772304755581376\n",
      "F1-Score: 0.7722666263111982\n"
     ]
    }
   ],
   "source": [
    " create_metric(\"Numerical\", y_pred_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Text Dataset \n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[40764 18736]\n",
      " [17134 43856]]\n",
      "\n",
      "\n",
      "Accuracy: 0.7022989459706199\n",
      "F1-Score: 0.702197305550473\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Text\", y_pred_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Weighted Dataset \n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[48385 11115]\n",
      " [12611 48379]]\n",
      "\n",
      "\n",
      "Accuracy: 0.8030873931446593\n",
      "F1-Score: 0.8030872713987556\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Weighted\", y_pred_stacking, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Sign-Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sign = pipelines.values[2]\n",
    "\n",
    "X_test = pipeline_sign.transformation.X_test\n",
    "X_test_numerical_transformed = pipeline_sign.transformation.X_test_numerical_transformed\n",
    "X_test_text_transformed = pipeline_sign.transformation.X_test_text_transformed\n",
    "y_test = pipeline_sign.transformation.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_num = pipeline_sign.modelling.numerical_model.predict(X_test_numerical_transformed)\n",
    "y_pred_text = pipeline_sign.modelling.text_model.predict(X_test_text_transformed)\n",
    "y_pred_stacking = pipeline_sign.modelling.weighted_prediction(X_test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric(y_pred_type, y_pred, y_test):\n",
    "    print(f\"Report for {y_pred_type} Dataset \\n\\n\")\n",
    "    \n",
    "    print(\"Relative Share of of each classes:\")\n",
    "    print(y_test.value_counts()/y_test.size)\n",
    "    print(\"The values above can be used to compare the model performance against with the most expected class\")\n",
    "    \n",
    "    print(f\"\\n\\nAccuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Numerical Dataset \n",
      "\n",
      "\n",
      "Relative Share of of each classes:\n",
      "Cancer         0.096730\n",
      "Aries          0.093742\n",
      "Libra          0.092041\n",
      "Taurus         0.090315\n",
      "Virgo          0.088613\n",
      "Scorpio        0.083468\n",
      "Pisces         0.080712\n",
      "Leo            0.078637\n",
      "Gemini         0.075749\n",
      "Aquarius       0.073749\n",
      "Sagittarius    0.073691\n",
      "Capricorn      0.072554\n",
      "Name: sign, dtype: float64\n",
      "The values above can be used to compare the model performance against with the most expected class\n",
      "\n",
      "\n",
      "Accuracy: 0.39932774504108226\n",
      "F1-Score: 0.40019804921862945\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Numerical\", y_pred_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Text Dataset \n",
      "\n",
      "\n",
      "Relative Share of of each classes:\n",
      "Cancer         0.096730\n",
      "Aries          0.093742\n",
      "Libra          0.092041\n",
      "Taurus         0.090315\n",
      "Virgo          0.088613\n",
      "Scorpio        0.083468\n",
      "Pisces         0.080712\n",
      "Leo            0.078637\n",
      "Gemini         0.075749\n",
      "Aquarius       0.073749\n",
      "Sagittarius    0.073691\n",
      "Capricorn      0.072554\n",
      "Name: sign, dtype: float64\n",
      "The values above can be used to compare the model performance against with the most expected class\n",
      "\n",
      "\n",
      "Accuracy: 0.20647356627105984\n",
      "F1-Score: 0.19900904971747324\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Text\", y_pred_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Stacking Dataset \n",
      "\n",
      "\n",
      "Relative Share of of each classes:\n",
      "Cancer         0.096730\n",
      "Aries          0.093742\n",
      "Libra          0.092041\n",
      "Taurus         0.090315\n",
      "Virgo          0.088613\n",
      "Scorpio        0.083468\n",
      "Pisces         0.080712\n",
      "Leo            0.078637\n",
      "Gemini         0.075749\n",
      "Aquarius       0.073749\n",
      "Sagittarius    0.073691\n",
      "Capricorn      0.072554\n",
      "Name: sign, dtype: float64\n",
      "The values above can be used to compare the model performance against with the most expected class\n",
      "\n",
      "\n",
      "Accuracy: 0.4213544692505602\n",
      "F1-Score: 0.4223261341341247\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Stacking\", y_pred_stacking, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Topic-Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_topic = pipelines.values[3]\n",
    "\n",
    "X_test = pipeline_topic.transformation.X_test\n",
    "X_test_numerical_transformed = pipeline_topic.transformation.X_test_numerical_transformed\n",
    "X_test_text_transformed = pipeline_topic.transformation.X_test_text_transformed\n",
    "y_test = pipeline_topic.transformation.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_num = pipeline_topic.modelling.numerical_model.predict(X_test_numerical_transformed)\n",
    "y_pred_text = pipeline_topic.modelling.text_model.predict(X_test_text_transformed)\n",
    "y_pred_stacking = pipeline_topic.modelling.weighted_prediction(X_test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Numerical Dataset \n",
      "\n",
      "\n",
      "Relative Share of of each classes:\n",
      "indUnk                     0.369881\n",
      "Student                    0.224865\n",
      "Technology                 0.061059\n",
      "Arts                       0.047788\n",
      "Education                  0.044020\n",
      "Communications-Media       0.030218\n",
      "Internet                   0.022450\n",
      "Non-Profit                 0.021413\n",
      "Engineering                0.017064\n",
      "Law                        0.013860\n",
      "Publishing                 0.011594\n",
      "Science                    0.011312\n",
      "Government                 0.010250\n",
      "Religion                   0.008382\n",
      "Consulting                 0.008333\n",
      "Fashion                    0.007013\n",
      "Marketing                  0.006897\n",
      "Advertising                0.006855\n",
      "BusinessServices           0.006449\n",
      "Banking                    0.005735\n",
      "Accounting                 0.005735\n",
      "Chemicals                  0.005212\n",
      "Telecommunications         0.004905\n",
      "Military                   0.004805\n",
      "Museums-Libraries          0.004681\n",
      "Sports-Recreation          0.004590\n",
      "RealEstate                 0.004432\n",
      "HumanResources             0.004349\n",
      "Transportation             0.003486\n",
      "Manufacturing              0.003436\n",
      "Biotech                    0.003087\n",
      "Tourism                    0.002855\n",
      "LawEnforcement-Security    0.002722\n",
      "Architecture               0.002033\n",
      "InvestmentBanking          0.001867\n",
      "Agriculture                0.001834\n",
      "Automotive                 0.001701\n",
      "Construction               0.001519\n",
      "Environment                0.000855\n",
      "Maritime                   0.000456\n",
      "Name: topic, dtype: float64\n",
      "The values above can be used to compare the model performance against with the most expected class\n",
      "\n",
      "\n",
      "Accuracy: 0.5580712092289817\n",
      "F1-Score: 0.5288272585345872\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Numerical\", y_pred_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Text Dataset \n",
      "\n",
      "\n",
      "Relative Share of of each classes:\n",
      "indUnk                     0.369881\n",
      "Student                    0.224865\n",
      "Technology                 0.061059\n",
      "Arts                       0.047788\n",
      "Education                  0.044020\n",
      "Communications-Media       0.030218\n",
      "Internet                   0.022450\n",
      "Non-Profit                 0.021413\n",
      "Engineering                0.017064\n",
      "Law                        0.013860\n",
      "Publishing                 0.011594\n",
      "Science                    0.011312\n",
      "Government                 0.010250\n",
      "Religion                   0.008382\n",
      "Consulting                 0.008333\n",
      "Fashion                    0.007013\n",
      "Marketing                  0.006897\n",
      "Advertising                0.006855\n",
      "BusinessServices           0.006449\n",
      "Banking                    0.005735\n",
      "Accounting                 0.005735\n",
      "Chemicals                  0.005212\n",
      "Telecommunications         0.004905\n",
      "Military                   0.004805\n",
      "Museums-Libraries          0.004681\n",
      "Sports-Recreation          0.004590\n",
      "RealEstate                 0.004432\n",
      "HumanResources             0.004349\n",
      "Transportation             0.003486\n",
      "Manufacturing              0.003436\n",
      "Biotech                    0.003087\n",
      "Tourism                    0.002855\n",
      "LawEnforcement-Security    0.002722\n",
      "Architecture               0.002033\n",
      "InvestmentBanking          0.001867\n",
      "Agriculture                0.001834\n",
      "Automotive                 0.001701\n",
      "Construction               0.001519\n",
      "Environment                0.000855\n",
      "Maritime                   0.000456\n",
      "Name: topic, dtype: float64\n",
      "The values above can be used to compare the model performance against with the most expected class\n",
      "\n",
      "\n",
      "Accuracy: 0.4311146153207735\n",
      "F1-Score: 0.37356421770434545\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Text\", y_pred_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Stacking Dataset \n",
      "\n",
      "\n",
      "Relative Share of of each classes:\n",
      "indUnk                     0.369881\n",
      "Student                    0.224865\n",
      "Technology                 0.061059\n",
      "Arts                       0.047788\n",
      "Education                  0.044020\n",
      "Communications-Media       0.030218\n",
      "Internet                   0.022450\n",
      "Non-Profit                 0.021413\n",
      "Engineering                0.017064\n",
      "Law                        0.013860\n",
      "Publishing                 0.011594\n",
      "Science                    0.011312\n",
      "Government                 0.010250\n",
      "Religion                   0.008382\n",
      "Consulting                 0.008333\n",
      "Fashion                    0.007013\n",
      "Marketing                  0.006897\n",
      "Advertising                0.006855\n",
      "BusinessServices           0.006449\n",
      "Banking                    0.005735\n",
      "Accounting                 0.005735\n",
      "Chemicals                  0.005212\n",
      "Telecommunications         0.004905\n",
      "Military                   0.004805\n",
      "Museums-Libraries          0.004681\n",
      "Sports-Recreation          0.004590\n",
      "RealEstate                 0.004432\n",
      "HumanResources             0.004349\n",
      "Transportation             0.003486\n",
      "Manufacturing              0.003436\n",
      "Biotech                    0.003087\n",
      "Tourism                    0.002855\n",
      "LawEnforcement-Security    0.002722\n",
      "Architecture               0.002033\n",
      "InvestmentBanking          0.001867\n",
      "Agriculture                0.001834\n",
      "Automotive                 0.001701\n",
      "Construction               0.001519\n",
      "Environment                0.000855\n",
      "Maritime                   0.000456\n",
      "Name: topic, dtype: float64\n",
      "The values above can be used to compare the model performance against with the most expected class\n",
      "\n",
      "\n",
      "Accuracy: 0.5589426508423936\n",
      "F1-Score: 0.5290997017535133\n"
     ]
    }
   ],
   "source": [
    "create_metric(\"Stacking\", y_pred_stacking, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
