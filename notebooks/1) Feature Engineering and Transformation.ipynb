{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# universally modules\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "\n",
    "# preprocessing and transformation modules\n",
    "import fasttext\n",
    "import Preprocessing\n",
    "from Features import buildFeatures\n",
    "from Modelling import StackingModelling\n",
    "from ModelSelection import ModelSelection, process_case\n",
    "from Transformation import StackedTransformation, transformation\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# model algorithm\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "\n",
    "# evaluation modules\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed up the process choose a sample size to randomly draw a sample of the whole daataset\n",
    "sample_mode = False\n",
    "sample_size = None \n",
    "\n",
    "# remove all text that contain less than n chars\n",
    "min_chars_per_text = 50\n",
    "\n",
    "# which features will be used for the TF-IDF transformation\n",
    "text_features = \"text_preprocessed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Preprocessing\n",
    "\n",
    "This part is independent from the cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../resource/data/blogtext.csv\")\n",
    "\n",
    "# draw random sample for faster processing:\n",
    "if sample_mode == True:\n",
    "    df = df.sample(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for a mininmal number of letters in a tweet:\n",
    "df = df[df[\"text\"].str.count(r\"[a-zA-Z]\") >= min_chars_per_text]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# transform to dask\n",
    "ds = dd.from_pandas(df[\"text\"], npartitions = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.8/site-packages/dask/dataframe/core.py:3199: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('text', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "# append the data\n",
    "features = list(ds.apply(buildFeatures).compute())\n",
    "\n",
    "# merge the features with the original dataset\n",
    "df_preprocessed = df.merge(pd.DataFrame(features), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.8/site-packages/distributed/worker.py:3376: UserWarning: Large object of size 74.84 MB detected in task graph: \n",
      "  (\"('from_pandas-5260b287f7aa7ba30ef88301e891ad74', ... 88301e891ad74')\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  warnings.warn(\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# use the preprocessing  module\n",
    "preprocessing = Preprocessing.Preprocessing()\n",
    "\n",
    "df_preprocessed[\"text_preprocessed\"] = ds.apply(preprocessing.ProcessOne, meta=(\"text\", str)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# predict the main language\n",
    "model = fasttext.load_model('../src/data/lid.176.ftz')\n",
    "df_preprocessed[\"main_language\"] = [model.predict(text)[0][0].split(\"__\")[-1] for text in df_preprocessed[\"text_preprocessed\"]]\n",
    "\n",
    "# drop unnecassary features\n",
    "df_filtered = df_preprocessed[(df_preprocessed[\"main_language\"] == \"en\")]\\\n",
    "                .drop([\"id\",\"text\",\"date\",\"main_language\"], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store df as pickle\n",
    "df_filtered.to_pickle(\"../resource/df_full_preprocessed.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
