{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from pandas_profiling import ProfileReport\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import Preprocessing\n",
    "import fasttext\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "df = pd.read_csv(\"../resource/data/blogtext.csv\", nrows= 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for a mininmal number of letters in a tweet:\n",
    "df = df[df[\"text\"].str.count(r\"[a-zA-Z]\") >= 10]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def findDates(text):\n",
    "#    try:\n",
    "#        return len([date for date in\\\n",
    "#                    datefinder.find_dates(text)])\n",
    "#    except:\n",
    "#        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatures(text):\n",
    "    text_split = text.split()\n",
    "    len_text = len(text)\n",
    "    sentence_split = tokenizer.tokenize(text)\n",
    "    \n",
    "    # find the number of urls in the text\n",
    "    keywords = [\"urlLink\",\"http\",\"www\"]\n",
    "    nb_urls = sum((any(keyword in pattern for keyword in keywords))\\\n",
    "               for pattern in text.split())\n",
    "    # find the number of mails in the text\n",
    "    nb_mails = len(re.findall(r\"([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+|\\bmail\\b)\"\\\n",
    "                      ,text))\n",
    "    \n",
    "    # find the number of dates in the text\n",
    "#    nb_dates = findDates(text)\n",
    "     \n",
    "    # find characteristics about the usage of letters, numbers and symbols\n",
    "    uppercase_ratio = len(re.findall(r'[A-Z]', text))/len_text\n",
    "    lowercase_ratio = len(re.findall(r'[a-z]', text))/len_text\n",
    "    number_ratio = len(re.findall(r'[0-9]', text))/len_text\n",
    "    symbol_ratio = len(re.findall(r'[$-/:-?{-~!\"^_`\\[\\]]', text))/len_text\n",
    "\n",
    "    # find characteristics about the letters per word\n",
    "    sentence_len_word = [len(word) for word in text_split]\n",
    "    avg_letters_per_word = np.mean([len(word) for word in text_split])\n",
    "    var_letters_per_word = np.var([len(word) for word in text_split])\n",
    "    unique_words_ratio = len(set(text_split))/len(text_split)\n",
    "\n",
    "    # find characteristics about the letters per sentence\n",
    "    sentence_len_list = [len(sentence) for sentence in sentence_split]\n",
    "    avg_letters_per_sentence = np.mean(sentence_len_list)\n",
    "    var_letters_per_sentence = np.var(sentence_len_list)\n",
    "    \n",
    "    # find characteristics about the words per sentence\n",
    "    words_per_sentence_len_list = [len(sentence.split()) for sentence in sentence_split]\n",
    "    avg_words_per_sentence = np.mean(words_per_sentence_len_list)\n",
    "    var_words_per_sentence = np.var(words_per_sentence_len_list)\n",
    "    \n",
    "    # find the trumps\n",
    "    uppercase_per_sentence_ratio = [len(re.findall(r'[A-Z]', sentence))/len(sentence)\\\n",
    "                                    for sentence in sentence_split]\n",
    "    max_sentence_uppercase_ratio = max(uppercase_per_sentence_ratio)\n",
    "    max_sentence_uppercase_len = len(sentence_split[uppercase_per_sentence_ratio.index(max_sentence_uppercase_ratio)])\n",
    "    \n",
    "    return len_text, nb_urls, nb_mails,\\\n",
    "           uppercase_ratio, lowercase_ratio, number_ratio, symbol_ratio,\\\n",
    "           avg_letters_per_word, var_letters_per_word, unique_words_ratio,\\\n",
    "           avg_letters_per_sentence, var_letters_per_sentence,\\\n",
    "           avg_words_per_sentence, var_words_per_sentence,\\\n",
    "           max_sentence_uppercase_ratio, max_sentence_uppercase_len        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9861/9861 [00:10<00:00, 924.85it/s] \n"
     ]
    }
   ],
   "source": [
    "# append the data\n",
    "features = [buildFeatures(text) for text  in tqdm(df[\"text\"])]\n",
    "\n",
    "# append the data\n",
    "columns = [\"Text length\", \"Number URLs\", \"Number mails\",\\\n",
    "          \"Uppercase ratio\", \"Lowercase ratio\", \"Number ratio\", \"Symbol ratio\",\\\n",
    "          \"Average letters per word\", \"Variance of letters per word\", \"Unique words ratio\",\\\n",
    "          \"Average letters per sentence\", \"Variance of letters per sentence\",\\\n",
    "          \"Average words per sentence\", \"Variance of words per sentence\",\\\n",
    "          \"Maximal uppercase ratio per sentence\", \"Length of the maximal uppercase ratio sentence\"]\n",
    "\n",
    "# merge the features with the original dataset\n",
    "df_preprocessed = df.merge(pd.DataFrame(features, columns=columns), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9861/9861 [03:53<00:00, 42.16it/s] \n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 9861/9861 [00:01<00:00, 8626.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# use the preprocessing  module\n",
    "preprocessor = Preprocessing.Preprocessing()\n",
    "df_preprocessed[\"text_preprocessed\"] = preprocessor.ProcessMany(df_preprocessed[\"text\"])\n",
    "\n",
    "# predict the main language\n",
    "model = fasttext.load_model('../src/data/lid.176.ftz')\n",
    "df_preprocessed[\"main_language\"] = [model.predict(text)[0][0].split(\"__\")[-1] for text in tqdm(df_preprocessed[\"text_preprocessed\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
