{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# universally modules\n",
    "import re\n",
    "import sys\n",
    "import regex\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# preprocessing and transformation modules\n",
    "import fasttext\n",
    "import Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# model algorithm\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# evaluation modules\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed up the process choose a sample size to randomly draw a sample of the whole daataset\n",
    "sample_size = 1000\n",
    "# remove all text that contain less than n chars\n",
    "min_chars_per_text = 50\n",
    "\n",
    "# which features will be used for the TF-IDF transformation\n",
    "text_features = \"text_preprocessed\"\n",
    "\n",
    "#### define the target variable and categorial variables used in later transformations ###\n",
    "#### Case 1: gender \n",
    "target_variable = \"gender\"\n",
    "categorial_variables =  [\"topic\", \"sign\"]\n",
    "\n",
    "# Case 2: topic\n",
    "#target_variable = \"topic\"\n",
    "#categorial_variables =  [\"gender\", \"sign\"]\n",
    "\n",
    "# Case 3: age\n",
    "#target_variable = \"age\"\n",
    "#categorial_variables =  [\"topic\", \"gender\", \"sign\"]\n",
    "\n",
    "# Case 4: sign\n",
    "#target_variable = \"sign\"\n",
    "#categorial_variables =  [\"gender\", \"topic\"]\n",
    "############################################################################################\n",
    "\n",
    "# use only words that occur at least sqrt_3(X) times \n",
    "min_df_exponent = (1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "df = pd.read_csv(\"../resource/data/blogtext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141352</th>\n",
       "      <td>2440626</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aries</td>\n",
       "      <td>17,July,2004</td>\n",
       "      <td>So Britt S. and I are starting a tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95697</th>\n",
       "      <td>751202</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>09,August,2003</td>\n",
       "      <td>AS THE SONG GOES...    urlLi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298286</th>\n",
       "      <td>1596986</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Biotech</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>17,May,2004</td>\n",
       "      <td>Aero replaces Luna in Longhorn. Doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447498</th>\n",
       "      <td>1409523</td>\n",
       "      <td>male</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>wow..... tonight was interesting.... ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618858</th>\n",
       "      <td>4263464</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>21,August,2004</td>\n",
       "      <td>urlLink    My daughter crawled f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597067</th>\n",
       "      <td>2994939</td>\n",
       "      <td>male</td>\n",
       "      <td>37</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Libra</td>\n",
       "      <td>27,May,2004</td>\n",
       "      <td>HIGHLIGHTS   INDIA – ECONOMY   India g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270572</th>\n",
       "      <td>3451304</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>Law</td>\n",
       "      <td>Libra</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>My Babies: Belle is on the left and Leo is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589506</th>\n",
       "      <td>3486663</td>\n",
       "      <td>female</td>\n",
       "      <td>13</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>10,July,2004</td>\n",
       "      <td>I laugh Wierd   If you notic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423994</th>\n",
       "      <td>1955799</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>Education</td>\n",
       "      <td>Libra</td>\n",
       "      <td>30,June,2004</td>\n",
       "      <td>Tonight and tomorrow I will work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319305</th>\n",
       "      <td>584088</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>Education</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>03,June,2002</td>\n",
       "      <td>urlLink Best Safire Editorial ever .  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age       topic     sign            date  \\\n",
       "141352  2440626    male   16     Student    Aries    17,July,2004   \n",
       "95697    751202  female   33    Internet   Gemini  09,August,2003   \n",
       "298286  1596986    male   23     Biotech   Pisces     17,May,2004   \n",
       "447498  1409523    male   17     Student  Scorpio  02,August,2004   \n",
       "618858  4263464    male   27  Technology   Pisces  21,August,2004   \n",
       "...         ...     ...  ...         ...      ...             ...   \n",
       "597067  2994939    male   37    Internet    Libra     27,May,2004   \n",
       "270572  3451304  female   25         Law    Libra    11,June,2004   \n",
       "589506  3486663  female   13     Student   Taurus    10,July,2004   \n",
       "423994  1955799    male   33   Education    Libra    30,June,2004   \n",
       "319305   584088    male   33   Education   Pisces    03,June,2002   \n",
       "\n",
       "                                                     text  \n",
       "141352             So Britt S. and I are starting a tr...  \n",
       "95697                     AS THE SONG GOES...    urlLi...  \n",
       "298286             Aero replaces Luna in Longhorn. Doe...  \n",
       "447498         wow..... tonight was interesting.... ye...  \n",
       "618858                urlLink    My daughter crawled f...  \n",
       "...                                                   ...  \n",
       "597067          HIGHLIGHTS   INDIA – ECONOMY   India g...  \n",
       "270572     My Babies: Belle is on the left and Leo is ...  \n",
       "589506                    I laugh Wierd   If you notic...  \n",
       "423994               Tonight and tomorrow I will work ...  \n",
       "319305          urlLink Best Safire Editorial ever .  ...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw random sample for faster processing:\n",
    "df = df.sample(sample_size)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for a mininmal number of letters in a tweet:\n",
    "df = df[df[\"text\"].str.count(r\"[a-zA-Z]\") >= min_chars_per_text]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def findDates(text):\n",
    "#    try:\n",
    "#        return len([date for date in\\\n",
    "#                    datefinder.find_dates(text)])\n",
    "#    except:\n",
    "#        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatures(text):\n",
    "    text_split = text.split()\n",
    "    len_text = len(text)\n",
    "    sentence_split = tokenizer.tokenize(text)\n",
    "    \n",
    "    # find the number of urls in the text\n",
    "    keywords = [\"urlLink\",\"http\",\"www\"]\n",
    "    nb_urls = sum((any(keyword in pattern for keyword in keywords))\\\n",
    "               for pattern in text.split())\n",
    "    # find the number of mails in the text\n",
    "    nb_mails = len(re.findall(r\"([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+|\\bmail\\b)\"\\\n",
    "                      ,text))\n",
    "    \n",
    "    # find the number of dates in the text\n",
    "#    nb_dates = findDates(text)\n",
    "     \n",
    "    # find characteristics about the usage of letters, numbers and symbols\n",
    "    uppercase_ratio = len(re.findall(r'[A-Z]', text))/len_text\n",
    "    lowercase_ratio = len(re.findall(r'[a-z]', text))/len_text\n",
    "    number_ratio = len(re.findall(r'[0-9]', text))/len_text\n",
    "    symbol_ratio = len(re.findall(r'[$-/:-?{-~!\"^_`\\[\\]]', text))/len_text\n",
    "\n",
    "    # find characteristics about the letters per word\n",
    "    sentence_len_word = [len(word) for word in text_split]\n",
    "    avg_letters_per_word = np.mean([len(word) for word in text_split])\n",
    "    var_letters_per_word = np.var([len(word) for word in text_split])\n",
    "    unique_words_ratio = len(set(text_split))/len(text_split)\n",
    "\n",
    "    # find characteristics about the letters per sentence\n",
    "    sentence_len_list = [len(sentence) for sentence in sentence_split]\n",
    "    avg_letters_per_sentence = np.mean(sentence_len_list)\n",
    "    var_letters_per_sentence = np.var(sentence_len_list)\n",
    "    \n",
    "    # find characteristics about the words per sentence\n",
    "    words_per_sentence_len_list = [len(sentence.split()) for sentence in sentence_split]\n",
    "    avg_words_per_sentence = np.mean(words_per_sentence_len_list)\n",
    "    var_words_per_sentence = np.var(words_per_sentence_len_list)\n",
    "    \n",
    "    # find the trumps\n",
    "    uppercase_per_sentence_ratio = [len(re.findall(r'[A-Z]', sentence))/len(sentence)\\\n",
    "                                    for sentence in sentence_split]\n",
    "    max_sentence_uppercase_ratio = max(uppercase_per_sentence_ratio)\n",
    "    max_sentence_uppercase_len = len(sentence_split[uppercase_per_sentence_ratio.index(max_sentence_uppercase_ratio)])\n",
    "    \n",
    "    return len_text, nb_urls, nb_mails,\\\n",
    "           uppercase_ratio, lowercase_ratio, number_ratio, symbol_ratio,\\\n",
    "           avg_letters_per_word, var_letters_per_word, unique_words_ratio,\\\n",
    "           avg_letters_per_sentence, var_letters_per_sentence,\\\n",
    "           avg_words_per_sentence, var_words_per_sentence,\\\n",
    "           max_sentence_uppercase_ratio, max_sentence_uppercase_len        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898/898 [00:01<00:00, 691.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# append the data\n",
    "features = [buildFeatures(text) for text  in tqdm(df[\"text\"])]\n",
    "\n",
    "# append the data\n",
    "columns = [\"Text length\", \"Number URLs\", \"Number mails\",\\\n",
    "          \"Uppercase ratio\", \"Lowercase ratio\", \"Number ratio\", \"Symbol ratio\",\\\n",
    "          \"Average letters per word\", \"Variance of letters per word\", \"Unique words ratio\",\\\n",
    "          \"Average letters per sentence\", \"Variance of letters per sentence\",\\\n",
    "          \"Average words per sentence\", \"Variance of words per sentence\",\\\n",
    "          \"Maximal uppercase ratio per sentence\", \"Length of the maximal uppercase ratio sentence\"]\n",
    "\n",
    "# merge the features with the original dataset\n",
    "df_preprocessed = df.merge(pd.DataFrame(features, columns=columns), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898/898 [00:27<00:00, 32.07it/s]\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 898/898 [00:00<00:00, 7072.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# use the preprocessing  module\n",
    "preprocessor = Preprocessing.Preprocessing()\n",
    "df_preprocessed[\"text_preprocessed\"] = preprocessor.ProcessMany(df_preprocessed[\"text\"])\n",
    "\n",
    "# predict the main language\n",
    "model = fasttext.load_model('../src/data/lid.176.ftz')\n",
    "df_preprocessed[\"main_language\"] = [model.predict(text)[0][0].split(\"__\")[-1] for text in tqdm(df_preprocessed[\"text_preprocessed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii_words(text):\n",
    "    return \"\".join([f\"{word} \" for word in text.split() if len(re.findall(r'[^\\x00-\\x7f]', word)) == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecassary features\n",
    "df_filtered = df_preprocessed[(df_preprocessed[\"main_language\"] == \"en\")]\\\n",
    "                .drop([\"id\",\"text\",\"date\",\"main_language\"], axis= 1)\n",
    "\n",
    "# remove all words with non-ascii chars\n",
    "df_filtered[\"text_preprocessed\"] = df_filtered[\"text_preprocessed\"].apply(remove_non_ascii_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedTransformation:\n",
    "    \n",
    "    def  __init__(self, X=None, y=None, numerical_transformer=None, text_transformer=None, text_features=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.numerical_transformer = numerical_transformer\n",
    "        self.text_transformer = text_transformer\n",
    "        self.text_features = text_features\n",
    "        \n",
    "    def data_split(self, X, y, test_size= 0.2):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "        \n",
    "    def build_transformer(self, X=None, y=None,\\\n",
    "                          numerical_transformer=None, text_transformer=None, text_features=None):\n",
    "        # check if variables are present\n",
    "        if X == None:\n",
    "            X = self.X\n",
    "        if y == None:\n",
    "            y = self.y\n",
    "        if numerical_transformer == None:\n",
    "            numerical_transformer = self.numerical_transformer\n",
    "        if text_transformer == None:\n",
    "            text_transformer = self.text_transformer\n",
    "        if text_features == None:\n",
    "            text_features = self.text_features\n",
    "\n",
    "        # split the data\n",
    "        \n",
    "        self.data_split(X, y)\n",
    "        \n",
    "        # Create datasets for each classifier\n",
    "        self.X_train_text = self.X_train[text_features]\n",
    "        self.X_test_text =  self.X_test[text_features]\n",
    "        \n",
    "        self.X_train_numerical = self.X_train.drop(text_features, axis=1)\n",
    "        self.X_test_numerical = self.X_test.drop(text_features, axis=1)\n",
    "\n",
    "        # create transformers \n",
    "        self.numerical_transformer = numerical_transformer\n",
    "        self.text_transformer = text_transformer\n",
    "        \n",
    "        #create transformed training batches\n",
    "        self.X_train_numerical_transformed = self.numerical_transformer.fit_transform(self.X_train_numerical)\n",
    "        self.X_test_numerical_transformed = self.numerical_transformer.transform(self.X_test_numerical)\n",
    "\n",
    "        self.X_train_text_transformed = self.text_transformer.fit_transform(self.X_train_text)\n",
    "        self.X_test_text_transformed = self.text_transformer.transform(self.X_test_text)\n",
    "\n",
    "    def transform_one(self, x):\n",
    "        entry = pd.DataFrame(columns=x.keys())\n",
    "        entry = entry.append(x, ignore_index=True)\n",
    "        \n",
    "        # create datasets for each transformer\n",
    "        entry_text = entry[text_features]\n",
    "        entry_numerical = entry.drop(text_features, axis=1)\n",
    "        \n",
    "        entry_numerical_transformed = self.numerical_transformer.transform(entry_numerical)\n",
    "        entry_text_transformed = self.text_transformer.transform(entry_text)\n",
    "        \n",
    "        return {\"transformed_text\": entry_text_transformed,\\\n",
    "               \"transformed_numerical\": entry_numerical_transformed}\n",
    "\n",
    "    def transform_one(self, x):\n",
    "        entry = pd.DataFrame(columns=x.keys())\n",
    "        entry = entry.append(x, ignore_index=True)\n",
    "        \n",
    "        # create datasets for each transformer\n",
    "        entry_text = entry[self.text_features]\n",
    "        entry_numerical = entry.drop(self.text_features, axis=1)\n",
    "        \n",
    "        entry_numerical_transformed = self.numerical_transformer.transform(entry_numerical)\n",
    "        entry_text_transformed = self.text_transformer.transform(entry_text)\n",
    "        \n",
    "        return {\"transformed_text\": entry_text_transformed,\\\n",
    "               \"transformed_numerical\": entry_numerical_transformed}\n",
    "\n",
    "\n",
    "    def transform_many(self, X):\n",
    "        # create datasets for each transformer\n",
    "        X_text = X[self.text_features]\n",
    "        X_numerical = X.drop(self.text_features, axis=1)\n",
    "        \n",
    "        X_numerical_transformed = self.numerical_transformer.transform(X_numerical)\n",
    "        X_text_transformed = self.text_transformer.transform(X_text)\n",
    "        \n",
    "        return {\"transformed_text\": X_text_transformed,\\\n",
    "               \"transformed_numerical\": X_numerical_transformed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df_filtered.drop(target_variable, axis=1),df_filtered[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the  text transformer class to create two transformers for the textual and the numerical model\n",
    "text_transformer = TfidfVectorizer(ngram_range=(1,1), min_df=int(len(X)**(min_df_exponent)))\n",
    "numerical_transformer = make_column_transformer((OneHotEncoder(handle_unknown=\"ignore\"), categorial_variables)\\\n",
    "                                                       , remainder=StandardScaler())\n",
    "\n",
    "stacking = StackedTransformation(X, y, numerical_transformer, text_transformer, text_features)\n",
    "stacking.build_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_data_features = stacking.text_transformer.get_feature_names()\n",
    "text_data = stacking.X_train_text_transformed.toarray()\n",
    "\n",
    "df_text_cluster = pd.DataFrame(text_data, columns=text_data_features)\n",
    "#df_text_cluster\n",
    "\n",
    "# Die Features beschreiben die Worte im Text\n",
    "# Die Werte sind die TF*IDF-transformierten Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    numerical_data = stacking.X_train_numerical_transformed.toarray()\n",
    "except:\n",
    "    numerical_data = stacking.X_train_numerical_transformed\n",
    "\n",
    "numerical_data_features = np.append(stacking.numerical_transformer.transformers_[0][1].get_feature_names(),\\\n",
    "                     stacking.X_train_numerical.columns.drop(categorial_variables))\n",
    "\n",
    "df_numerical_cluster = pd.DataFrame(numerical_data, columns=numerical_data_features)\n",
    "#df_numerical_cluster\n",
    "\n",
    "\n",
    "# Die Features mit den x0 - xi Werten beschreiben die Ausprägungen die kategorialen Variablen\n",
    "# das jeweilige i beschreibt das i-te Element der im Punkt \"Target Variable\" definierten liste categorial_variables\n",
    "# Die verbleibenden Features (ohne xi) sind Standardskaliert, (x - \\mu)/\\sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingModelling:\n",
    "    \n",
    "    \n",
    "    def __init__(self,  numerical_model, numerical_model_params,\\\n",
    "                text_model, text_model_params, stacked_transformation_instance,\\\n",
    "                weights=(0.5, 0.5)):\n",
    "        \n",
    "        # Initialize numerical model\n",
    "        self.numerical_model = numerical_model(**numerical_model_params)\n",
    "        \n",
    "        # Initialize text model\n",
    "        self.text_model = text_model(**text_model_params)\n",
    "        \n",
    "        # internalize transformation class\n",
    "        self.stacked_transformation = stacked_transformation_instance\n",
    "        \n",
    "        # model weights\n",
    "        self.weights = weights\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        # train numerical model\n",
    "        self.numerical_model.fit(self.stacked_transformation.X_train_numerical_transformed,\\\n",
    "                             self.stacked_transformation.y_train)\n",
    "                             \n",
    "        print(\"Numerical model finished!\")\n",
    "        \n",
    "        # train textual model\n",
    "        self.text_model.fit(self.stacked_transformation.X_train_text_transformed,\\\n",
    "                             self.stacked_transformation.y_train)\n",
    "                             \n",
    "        print(\"Text model finished!\")\n",
    "\n",
    "        \n",
    "    def predict_text(self, X):\n",
    "        return self.text_model.predict(X)\n",
    "    \n",
    "    \n",
    "    def predict_numerical(self, X):\n",
    "        return self.numerical_model.predict(X)\n",
    "\n",
    "    \n",
    "    def optimize_weights(self, X, y, algo_type=\"classification\"):\n",
    "        X_transformed = self.stacked_transformation.transform_many(X)\n",
    "\n",
    "        if algo_type == \"classification\":\n",
    "            class_label_dict = dict([(k,v) for v,k in enumerate(self.numerical_model.classes_)])\n",
    "            y_class_index = [class_label_dict[value] for value in y]\n",
    "            \n",
    "            # predict each row of X for each model\n",
    "            y_pred_num = self.numerical_model.predict_proba(X_transformed[\"transformed_numerical\"])\n",
    "            y_pred_text = self.text_model.predict_proba(X_transformed[\"transformed_text\"])\n",
    "            \n",
    "            # absolute loss for each model\n",
    "            self.loss_numerical = sum([sum(np.delete(y_p, y_t)) for y_p, y_t in zip(y_pred_num,y_class_index)])\n",
    "            self.loss_text = sum([sum(np.delete(y_p, y_t)) for y_p, y_t in zip(y_pred_text,y_class_index)])\n",
    "            \n",
    "            # optimize the weights based on their contribution to the loss\n",
    "            # bewusst text und numerical vertauscht, damit die Gegenwahrscheinlichkeit verwendet wird.\n",
    "            self.weights = np.array((self.loss_numerical, self.loss_text))/(self.loss_numerical +self.loss_text)\n",
    "            \n",
    "\n",
    "        if algo_type == \"regression\":\n",
    "            # absolute loss for each model\n",
    "            self.loss_numerical = np.absolute(self.numerical_model.predict(X_transformed[\"transformed_numerical\"]) - y).sum()\n",
    "            self.loss_text = np.absolute(self.text_model.predict(X_transformed[\"transformed_text\"]) - y).sum()\n",
    "            \n",
    "            # optimize the weights based on their contribution to the loss\n",
    "            # bewusst text und numerical vertauscht, damit die Gegenwahrscheinlichkeit verwendet wird.\n",
    "            self.weights = np.array((self.loss_numerical, self.loss_text))/(self.loss_numerical + self.loss_text)\n",
    "            \n",
    "            \n",
    "        print(f\"\"\"Weights have been optimized:\n",
    "                Textual model weight: {self.weights[0]}\n",
    "                Numerical model weight: {self.weights[1]}\"\"\")\n",
    "\n",
    "    def weighted_prediction(self, X, weights=None, algo_type=\"classification\"):\n",
    "        if weights == None:\n",
    "            weights = self.weights\n",
    "        \n",
    "        # check if one or more transactions become processed\n",
    "        try:\n",
    "            if type(X) == dict:\n",
    "                X_transformed = self.stacked_transformation.transform_one(X)\n",
    "\n",
    "            else:\n",
    "                X_transformed = self.stacked_transformation.transform_many(X)\n",
    "        except TypeError:\n",
    "            print(\"Check the data type of the input data\")\n",
    "\n",
    "        if algo_type == \"classification\":\n",
    "            predictions = (self.text_model.predict_proba(X_transformed[\"transformed_text\"]),\\\n",
    "                            self.numerical_model.predict_proba(X_transformed[\"transformed_numerical\"]))\n",
    "            \n",
    "            index = lambda x: np.argmax(x)\n",
    "            \n",
    "            self.weighted_predictions = (predictions[0]*weights[0] + predictions[1]*weights[1])\n",
    "            classes_name = np.array([self.text_model.classes_[index(prediction)] for prediction in self.weighted_predictions])\n",
    "            \n",
    "            # get name of the classes\n",
    "            return classes_name\n",
    "        \n",
    "        if algo_type == \"regression\":\n",
    "            predictions = (self.text_model.predict(X_transformed[\"transformed_text\"]),\\\n",
    "                            self.numerical_model.predict(X_transformed[\"transformed_numerical\"]))\n",
    "                        \n",
    "            self.weighted_predictions = (predictions[0]*weights[0] + predictions[1]*weights[1])\n",
    "            \n",
    "            return self.weighted_predictions\n",
    "        \n",
    "        else:\n",
    "            print(\"target variable type not supported\")\n",
    "            \n",
    "    def create_report(self, X, y, algo_type=\"classification\"):\n",
    "        # fit weight optimization\n",
    "        self.optimize_weights(X,y, algo_type)\n",
    "        \n",
    "        #get loss for bagged models:\n",
    "        if algo_type == \"classification\":\n",
    "            class_label_dict = dict([(k,v) for v,k in enumerate(self.numerical_model.classes_)])\n",
    "            y_class_index = [class_label_dict[value] for value in y]\n",
    "            \n",
    "            # get loss for unoptimized weighting\n",
    "            self.weighted_prediction(X, (0.5,0.5), algo_type=\"classification\")\n",
    "            y_pred = self.weighted_predictions\n",
    "            loss_weights = sum([sum(np.delete(y_p, y_t)) for y_p, y_t in zip(y_pred,y_class_index)])\n",
    "            \n",
    "            # get loss for optimized weights\n",
    "            self.weighted_prediction(X, algo_type=\"classification\")\n",
    "            y_pred = self.weighted_predictions\n",
    "            loss_weights_optimized = sum([sum(np.delete(y_p, y_t)) for y_p, y_t in zip(y_pred,y_class_index)])\n",
    "                        \n",
    "        self.report = pd.Series()\n",
    "        self.report[\"Absolute loss textual model\"] = self.loss_text\n",
    "        self.report[\"Absolute loss numerical model\"] = self.loss_numerical\n",
    "        self.report[\"Absolute loss equally weighted model\"] = loss_weights\n",
    "        self.report[\"Absolute loss optimized weights model\"] = loss_weights_optimized\n",
    "\n",
    "        return self.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical model finished!\n",
      "Text model finished!\n",
      "Weights have been optimized:\n",
      "                Textual model weight: 0.49170281196854165\n",
      "                Numerical model weight: 0.5082971880314584\n",
      "Weights have been optimized:\n",
      "                Textual model weight: 0.49170281196854165\n",
      "                Numerical model weight: 0.5082971880314584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-0d11a793c39d>:133: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  self.report = pd.Series()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Absolute loss textual model              84.142973\n",
       "Absolute loss numerical model            81.395958\n",
       "Absolute loss equally weighted model     82.769465\n",
       "Absolute loss optimized weights model    82.746675\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_type = \"classification\"\n",
    "stacked_modelling = BaggingModelling(XGBClassifier, {}, XGBClassifier, {}, stacking, (0.5, 0.5))\n",
    "X_test = stacked_modelling.stacked_transformation.X_test\n",
    "y_test = stacked_modelling.stacked_transformation.y_test\n",
    "\n",
    "\n",
    "stacked_modelling.fit()\n",
    "stacked_modelling.optimize_weights(X_test, y_test, algo_type=algo_type)\n",
    "stacked_modelling.create_report(X_test,y_test, algo_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
