{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# universally modules\n",
    "import re\n",
    "import sys\n",
    "import regex\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# preprocessing and transformation modules\n",
    "import fasttext\n",
    "import Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# model algorithm\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# evaluation modules\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed up the process choose a sample size to randomly draw a sample of the whole daataset\n",
    "sample_size = 100\n",
    "# remove all text that contain less than n chars\n",
    "min_chars_per_text = 50\n",
    "\n",
    "# which features will be used for the TF-IDF transformation\n",
    "text_features = \"text_preprocessed\"\n",
    "\n",
    "#### define the target variable and categorial variables used in later transformations ###\n",
    "#### Case 1: gender \n",
    "target_variable = \"gender\"\n",
    "categorial_variables =  [\"topic\", \"sign\"]\n",
    "\n",
    "# Case 2: topic\n",
    "#target_variable = \"topic\"\n",
    "#categorial_variables =  [\"gender\", \"sign\"]\n",
    "\n",
    "# Case 3: age\n",
    "#target_variable = \"age\"\n",
    "#categorial_variables =  [\"topic\", \"gender\", \"sign\"]\n",
    "\n",
    "# Case 4: sign\n",
    "#target_variable = \"sign\"\n",
    "#categorial_variables =  [\"gender\", \"topic\"]\n",
    "############################################################################################\n",
    "\n",
    "# use only words that occur at least sqrt_3(X) times \n",
    "min_df_exponent = (1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "df = pd.read_csv(\"../resource/data/blogtext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>251194</th>\n",
       "      <td>3610960</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aries</td>\n",
       "      <td>12,July,2004</td>\n",
       "      <td>they're long, complicated, nd som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653185</th>\n",
       "      <td>1015556</td>\n",
       "      <td>male</td>\n",
       "      <td>34</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>11,December,2003</td>\n",
       "      <td>Wow.  Got some seriously cool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524296</th>\n",
       "      <td>1960555</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>01,August,2004</td>\n",
       "      <td>Music:  Foo Fighters x Everlong  Mood:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136724</th>\n",
       "      <td>4001907</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>Aries</td>\n",
       "      <td>23,July,2004</td>\n",
       "      <td>urlLink    There have me and a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337212</th>\n",
       "      <td>3983169</td>\n",
       "      <td>male</td>\n",
       "      <td>17</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Aries</td>\n",
       "      <td>22,July,2004</td>\n",
       "      <td>Blah.&amp;nbsp; Its kinda early.&amp;nbsp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43520</th>\n",
       "      <td>1713442</td>\n",
       "      <td>male</td>\n",
       "      <td>34</td>\n",
       "      <td>Education</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>20,August,2003</td>\n",
       "      <td>urlLink Tales from the MSU Stacks  I did a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517506</th>\n",
       "      <td>180519</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>28,March,2002</td>\n",
       "      <td>slipping into a funk again--my mind fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317127</th>\n",
       "      <td>1432868</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>15,September,2003</td>\n",
       "      <td>'Aoccdrnig to a rscheearch at an El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61260</th>\n",
       "      <td>2871824</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Leo</td>\n",
       "      <td>19,June,2004</td>\n",
       "      <td>Yesterday very busy until me always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315061</th>\n",
       "      <td>216413</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>Law</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>14,February,2003</td>\n",
       "      <td>last night, someone almost ruined my he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age        topic       sign               date  \\\n",
       "251194  3610960  female   16      Student      Aries       12,July,2004   \n",
       "653185  1015556    male   34   Technology      Virgo   11,December,2003   \n",
       "524296  1960555  female   16      Student  Capricorn     01,August,2004   \n",
       "136724  4001907    male   27   Publishing      Aries       23,July,2004   \n",
       "337212  3983169    male   17       indUnk      Aries       22,July,2004   \n",
       "...         ...     ...  ...          ...        ...                ...   \n",
       "43520   1713442    male   34    Education   Aquarius     20,August,2003   \n",
       "517506   180519  female   23       indUnk     Cancer      28,March,2002   \n",
       "317127  1432868    male   16      Student    Scorpio  15,September,2003   \n",
       "61260   2871824    male   24  Engineering        Leo       19,June,2004   \n",
       "315061   216413  female   26          Law     Cancer   14,February,2003   \n",
       "\n",
       "                                                     text  \n",
       "251194               they're long, complicated, nd som...  \n",
       "653185                   Wow.  Got some seriously cool...  \n",
       "524296          Music:  Foo Fighters x Everlong  Mood:...  \n",
       "136724                urlLink    There have me and a c...  \n",
       "337212              Blah.&nbsp; Its kinda early.&nbsp;...  \n",
       "...                                                   ...  \n",
       "43520       urlLink Tales from the MSU Stacks  I did a...  \n",
       "517506         slipping into a funk again--my mind fee...  \n",
       "317127             'Aoccdrnig to a rscheearch at an El...  \n",
       "61260              Yesterday very busy until me always...  \n",
       "315061         last night, someone almost ruined my he...  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw random sample for faster processing:\n",
    "df = df.sample(sample_size)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for a mininmal number of letters in a tweet:\n",
    "df = df[df[\"text\"].str.count(r\"[a-zA-Z]\") >= min_chars_per_text]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def findDates(text):\n",
    "#    try:\n",
    "#        return len([date for date in\\\n",
    "#                    datefinder.find_dates(text)])\n",
    "#    except:\n",
    "#        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatures(text):\n",
    "    text_split = text.split()\n",
    "    len_text = len(text)\n",
    "    sentence_split = tokenizer.tokenize(text)\n",
    "    \n",
    "    # find the number of urls in the text\n",
    "    keywords = [\"urlLink\",\"http\",\"www\"]\n",
    "    nb_urls = sum((any(keyword in pattern for keyword in keywords))\\\n",
    "               for pattern in text.split())\n",
    "    # find the number of mails in the text\n",
    "    nb_mails = len(re.findall(r\"([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+|\\bmail\\b)\"\\\n",
    "                      ,text))\n",
    "    \n",
    "    # find the number of dates in the text\n",
    "#    nb_dates = findDates(text)\n",
    "     \n",
    "    # find characteristics about the usage of letters, numbers and symbols\n",
    "    uppercase_ratio = len(re.findall(r'[A-Z]', text))/len_text\n",
    "    lowercase_ratio = len(re.findall(r'[a-z]', text))/len_text\n",
    "    number_ratio = len(re.findall(r'[0-9]', text))/len_text\n",
    "    symbol_ratio = len(re.findall(r'[$-/:-?{-~!\"^_`\\[\\]]', text))/len_text\n",
    "\n",
    "    # find characteristics about the letters per word\n",
    "    sentence_len_word = [len(word) for word in text_split]\n",
    "    avg_letters_per_word = np.mean([len(word) for word in text_split])\n",
    "    var_letters_per_word = np.var([len(word) for word in text_split])\n",
    "    unique_words_ratio = len(set(text_split))/len(text_split)\n",
    "\n",
    "    # find characteristics about the letters per sentence\n",
    "    sentence_len_list = [len(sentence) for sentence in sentence_split]\n",
    "    avg_letters_per_sentence = np.mean(sentence_len_list)\n",
    "    var_letters_per_sentence = np.var(sentence_len_list)\n",
    "    \n",
    "    # find characteristics about the words per sentence\n",
    "    words_per_sentence_len_list = [len(sentence.split()) for sentence in sentence_split]\n",
    "    avg_words_per_sentence = np.mean(words_per_sentence_len_list)\n",
    "    var_words_per_sentence = np.var(words_per_sentence_len_list)\n",
    "    \n",
    "    # find the trumps\n",
    "    uppercase_per_sentence_ratio = [len(re.findall(r'[A-Z]', sentence))/len(sentence)\\\n",
    "                                    for sentence in sentence_split]\n",
    "    max_sentence_uppercase_ratio = max(uppercase_per_sentence_ratio)\n",
    "    max_sentence_uppercase_len = len(sentence_split[uppercase_per_sentence_ratio.index(max_sentence_uppercase_ratio)])\n",
    "    \n",
    "    return len_text, nb_urls, nb_mails,\\\n",
    "           uppercase_ratio, lowercase_ratio, number_ratio, symbol_ratio,\\\n",
    "           avg_letters_per_word, var_letters_per_word, unique_words_ratio,\\\n",
    "           avg_letters_per_sentence, var_letters_per_sentence,\\\n",
    "           avg_words_per_sentence, var_words_per_sentence,\\\n",
    "           max_sentence_uppercase_ratio, max_sentence_uppercase_len        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 800.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# append the data\n",
    "features = [buildFeatures(text) for text  in tqdm(df[\"text\"])]\n",
    "\n",
    "# append the data\n",
    "columns = [\"Text length\", \"Number URLs\", \"Number mails\",\\\n",
    "          \"Uppercase ratio\", \"Lowercase ratio\", \"Number ratio\", \"Symbol ratio\",\\\n",
    "          \"Average letters per word\", \"Variance of letters per word\", \"Unique words ratio\",\\\n",
    "          \"Average letters per sentence\", \"Variance of letters per sentence\",\\\n",
    "          \"Average words per sentence\", \"Variance of words per sentence\",\\\n",
    "          \"Maximal uppercase ratio per sentence\", \"Length of the maximal uppercase ratio sentence\"]\n",
    "\n",
    "# merge the features with the original dataset\n",
    "df_preprocessed = df.merge(pd.DataFrame(features, columns=columns), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:02<00:00, 32.82it/s]\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 95/95 [00:00<00:00, 8004.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# use the preprocessing  module\n",
    "preprocessor = Preprocessing.Preprocessing()\n",
    "df_preprocessed[\"text_preprocessed\"] = preprocessor.ProcessMany(df_preprocessed[\"text\"])\n",
    "\n",
    "# predict the main language\n",
    "model = fasttext.load_model('../src/data/lid.176.ftz')\n",
    "df_preprocessed[\"main_language\"] = [model.predict(text)[0][0].split(\"__\")[-1] for text in tqdm(df_preprocessed[\"text_preprocessed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii_words(text):\n",
    "    return \"\".join([f\"{word} \" for word in text.split() if len(re.findall(r'[^\\x00-\\x7f]', word)) == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecassary features\n",
    "df_filtered = df_preprocessed[(df_preprocessed[\"main_language\"] == \"en\")]\\\n",
    "                .drop([\"id\",\"text\",\"date\",\"main_language\"], axis= 1)\n",
    "\n",
    "# remove all words with non-ascii chars\n",
    "df_filtered[\"text_preprocessed\"] = df_filtered[\"text_preprocessed\"].apply(remove_non_ascii_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedTransformation:\n",
    "    \n",
    "    def  __init__(self, X=None, y=None, numerical_transformer=None, text_transformer=None, text_features=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.numerical_transformer = numerical_transformer\n",
    "        self.text_transformer = text_transformer\n",
    "        self.text_features = text_features\n",
    "        \n",
    "    def data_split(self, X, y, test_size= 0.2):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "        \n",
    "    def build_transformer(self, X=None, y=None,\\\n",
    "                          numerical_transformer=None, text_transformer=None, text_features=None):\n",
    "        # check if variables are present\n",
    "        if X == None:\n",
    "            X = self.X\n",
    "        if y == None:\n",
    "            y = self.y\n",
    "        if numerical_transformer == None:\n",
    "            numerical_transformer = self.numerical_transformer\n",
    "        if text_transformer == None:\n",
    "            text_transformer = self.text_transformer\n",
    "        if text_features == None:\n",
    "            text_features = self.text_features\n",
    "\n",
    "        # split the data\n",
    "        \n",
    "        self.data_split(X, y)\n",
    "        \n",
    "        # Create datasets for each classifier\n",
    "        self.X_train_text = self.X_train[text_features]\n",
    "        self.X_test_text =  self.X_test[text_features]\n",
    "        \n",
    "        self.X_train_numerical = self.X_train.drop(text_features, axis=1)\n",
    "        self.X_test_numerical = self.X_test.drop(text_features, axis=1)\n",
    "\n",
    "        # create transformers \n",
    "        self.numerical_transformer = numerical_transformer\n",
    "        self.text_transformer = text_transformer\n",
    "        \n",
    "        #create transformed training batches\n",
    "        self.X_train_numerical_transformed = self.numerical_transformer.fit_transform(self.X_train_numerical)\n",
    "        self.X_test_numerical_transformed = self.numerical_transformer.transform(self.X_test_numerical)\n",
    "\n",
    "        self.X_train_text_transformed = self.text_transformer.fit_transform(self.X_train_text)\n",
    "        self.X_test_text_transformed = self.text_transformer.transform(self.X_test_text)\n",
    "\n",
    "    def transform_one(self, x):\n",
    "        entry = pd.DataFrame(columns=x.keys())\n",
    "        entry = entry.append(x, ignore_index=True)\n",
    "        \n",
    "        # create datasets for each transformer\n",
    "        entry_text = entry[text_features]\n",
    "        entry_numerical = entry.drop(text_features, axis=1)\n",
    "        \n",
    "        entry_numerical_transformed = self.numerical_transformer.transform(entry_numerical)\n",
    "        entry_text_transformed = self.text_transformer.transform(entry_text)\n",
    "        \n",
    "        return {\"transformed_text\": entry_text_transformed,\\\n",
    "               \"transformed_numerical\": entry_numerical_transformed}\n",
    "\n",
    "    def transform_one(self, x):\n",
    "        entry = pd.DataFrame(columns=x.keys())\n",
    "        entry = entry.append(x, ignore_index=True)\n",
    "        \n",
    "        # create datasets for each transformer\n",
    "        entry_text = entry[self.text_features]\n",
    "        entry_numerical = entry.drop(self.text_features, axis=1)\n",
    "        \n",
    "        entry_numerical_transformed = self.numerical_transformer.transform(entry_numerical)\n",
    "        entry_text_transformed = self.text_transformer.transform(entry_text)\n",
    "        \n",
    "        return {\"transformed_text\": entry_text_transformed,\\\n",
    "               \"transformed_numerical\": entry_numerical_transformed}\n",
    "\n",
    "\n",
    "    def transform_many(self, X):\n",
    "        # create datasets for each transformer\n",
    "        X_text = X[self.text_features]\n",
    "        X_numerical = X.drop(self.text_features, axis=1)\n",
    "        \n",
    "        X_numerical_transformed = self.numerical_transformer.transform(X_numerical)\n",
    "        X_text_transformed = self.text_transformer.transform(X_text)\n",
    "        \n",
    "        return {\"transformed_text\": X_text_transformed,\\\n",
    "               \"transformed_numerical\": X_numerical_transformed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df_filtered.drop(target_variable, axis=1),df_filtered[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the  text transformer class to create two transformers for the textual and the numerical model\n",
    "text_transformer = TfidfVectorizer(ngram_range=(1,1), min_df=int(len(X)**(min_df_exponent)))\n",
    "numerical_transformer = make_column_transformer((OneHotEncoder(handle_unknown=\"ignore\"), categorial_variables)\\\n",
    "                                                       , remainder=StandardScaler())\n",
    "\n",
    "stacking = StackedTransformation(X, y, numerical_transformer, text_transformer, text_features)\n",
    "stacking.build_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>30</th>\n",
       "      <th>ask</th>\n",
       "      <th>back</th>\n",
       "      <th>bad</th>\n",
       "      <th>be</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>bed</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>...</th>\n",
       "      <th>wo</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>wow</th>\n",
       "      <th>write</th>\n",
       "      <th>yay</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342544</td>\n",
       "      <td>0.486032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.239198</td>\n",
       "      <td>0.110719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.061253</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.064042</td>\n",
       "      <td>0.045196</td>\n",
       "      <td>0.047461</td>\n",
       "      <td>0.289328</td>\n",
       "      <td>0.064042</td>\n",
       "      <td>0.128085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250010</td>\n",
       "      <td>0.264655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.387706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 238 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          10        30       ask      back       bad        be  beautiful  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.084975  0.000000   0.000000   \n",
       "2   0.000000  0.000000  0.000000  0.044264  0.000000  0.035420   0.000000   \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "..       ...       ...       ...       ...       ...       ...        ...   \n",
       "70  0.239198  0.110719  0.000000  0.000000  0.000000  0.070616   0.000000   \n",
       "71  0.000000  0.000000  0.000000  0.000000  0.000000  0.124626   0.000000   \n",
       "72  0.061253  0.056705  0.064042  0.045196  0.047461  0.289328   0.064042   \n",
       "73  0.000000  0.000000  0.250010  0.264655  0.000000  0.000000   0.000000   \n",
       "74  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "         bed  big       bit  ...        wo  word      work     world  wow  \\\n",
       "0   0.000000  0.0  0.000000  ...  0.000000   0.0  0.000000  0.000000  0.0   \n",
       "1   0.000000  0.0  0.000000  ...  0.000000   0.0  0.000000  0.000000  0.0   \n",
       "2   0.000000  0.0  0.000000  ...  0.000000   0.0  0.000000  0.057623  0.0   \n",
       "3   0.000000  0.0  0.154424  ...  0.000000   0.0  0.000000  0.000000  0.0   \n",
       "4   0.000000  0.0  0.000000  ...  0.000000   0.0  0.000000  0.000000  0.0   \n",
       "..       ...  ...       ...  ...       ...   ...       ...       ...  ...   \n",
       "70  0.000000  0.0  0.000000  ...  0.000000   0.0  0.232699  0.000000  0.0   \n",
       "71  0.000000  0.0  0.000000  ...  0.000000   0.0  0.000000  0.000000  0.0   \n",
       "72  0.128085  0.0  0.000000  ...  0.000000   0.0  0.119177  0.000000  0.0   \n",
       "73  0.000000  0.0  0.000000  ...  0.125005   0.0  0.387706  0.000000  0.0   \n",
       "74  0.000000  0.0  0.000000  ...  0.000000   0.0  0.000000  0.000000  0.0   \n",
       "\n",
       "       write  yay      yeah      year  yesterday  \n",
       "0   0.000000  0.0  0.000000  0.000000   0.000000  \n",
       "1   0.000000  0.0  0.329005  0.000000   0.000000  \n",
       "2   0.000000  0.0  0.000000  0.000000   0.000000  \n",
       "3   0.000000  0.0  0.000000  0.000000   0.000000  \n",
       "4   0.000000  0.0  0.000000  0.342544   0.486032  \n",
       "..       ...  ...       ...       ...        ...  \n",
       "70  0.000000  0.0  0.000000  0.000000   0.000000  \n",
       "71  0.000000  0.0  0.211073  0.000000   0.000000  \n",
       "72  0.000000  0.0  0.061253  0.000000   0.000000  \n",
       "73  0.106962  0.0  0.000000  0.000000   0.131445  \n",
       "74  0.000000  0.0  0.000000  0.000000   0.000000  \n",
       "\n",
       "[75 rows x 238 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_features = stacking.text_transformer.get_feature_names()\n",
    "text_data = stacking.X_train_text_transformed.toarray()\n",
    "\n",
    "df_text_cluster = pd.DataFrame(text_data, columns=text_data_features)\n",
    "df_text_cluster\n",
    "\n",
    "# Die Features beschreiben die Worte im Text\n",
    "# Die Werte sind die TF*IDF-transformierten Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_Accounting</th>\n",
       "      <th>x0_Agriculture</th>\n",
       "      <th>x0_Arts</th>\n",
       "      <th>x0_Communications-Media</th>\n",
       "      <th>x0_Education</th>\n",
       "      <th>x0_Engineering</th>\n",
       "      <th>x0_Fashion</th>\n",
       "      <th>x0_Internet</th>\n",
       "      <th>x0_InvestmentBanking</th>\n",
       "      <th>x0_Law</th>\n",
       "      <th>...</th>\n",
       "      <th>Symbol ratio</th>\n",
       "      <th>Average letters per word</th>\n",
       "      <th>Variance of letters per word</th>\n",
       "      <th>Unique words ratio</th>\n",
       "      <th>Average letters per sentence</th>\n",
       "      <th>Variance of letters per sentence</th>\n",
       "      <th>Average words per sentence</th>\n",
       "      <th>Variance of words per sentence</th>\n",
       "      <th>Maximal uppercase ratio per sentence</th>\n",
       "      <th>Length of the maximal uppercase ratio sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.728211</td>\n",
       "      <td>-0.373913</td>\n",
       "      <td>-0.405588</td>\n",
       "      <td>-1.034518</td>\n",
       "      <td>-0.172704</td>\n",
       "      <td>-0.172048</td>\n",
       "      <td>-0.114345</td>\n",
       "      <td>-0.149941</td>\n",
       "      <td>-0.426713</td>\n",
       "      <td>-0.508643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441599</td>\n",
       "      <td>0.662686</td>\n",
       "      <td>-0.225826</td>\n",
       "      <td>0.630411</td>\n",
       "      <td>-0.519892</td>\n",
       "      <td>-0.226515</td>\n",
       "      <td>-0.509414</td>\n",
       "      <td>-0.208545</td>\n",
       "      <td>3.848010</td>\n",
       "      <td>-0.592957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215583</td>\n",
       "      <td>1.093433</td>\n",
       "      <td>-0.397563</td>\n",
       "      <td>1.570226</td>\n",
       "      <td>-0.562348</td>\n",
       "      <td>-0.256236</td>\n",
       "      <td>-0.608481</td>\n",
       "      <td>-0.254289</td>\n",
       "      <td>-0.491851</td>\n",
       "      <td>-0.068333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613411</td>\n",
       "      <td>-0.303795</td>\n",
       "      <td>-0.514383</td>\n",
       "      <td>0.885276</td>\n",
       "      <td>-0.344406</td>\n",
       "      <td>-0.201200</td>\n",
       "      <td>-0.292459</td>\n",
       "      <td>-0.192371</td>\n",
       "      <td>-0.337887</td>\n",
       "      <td>-0.424328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.947118</td>\n",
       "      <td>1.279772</td>\n",
       "      <td>-0.381717</td>\n",
       "      <td>1.409860</td>\n",
       "      <td>2.008931</td>\n",
       "      <td>-0.266308</td>\n",
       "      <td>1.353031</td>\n",
       "      <td>-0.254289</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>2.030163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155387</td>\n",
       "      <td>0.424941</td>\n",
       "      <td>-0.135984</td>\n",
       "      <td>1.149267</td>\n",
       "      <td>-0.463039</td>\n",
       "      <td>-0.247183</td>\n",
       "      <td>-0.499508</td>\n",
       "      <td>-0.251388</td>\n",
       "      <td>-0.330291</td>\n",
       "      <td>0.081559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.791656</td>\n",
       "      <td>0.483748</td>\n",
       "      <td>-0.272493</td>\n",
       "      <td>-0.283885</td>\n",
       "      <td>-0.004562</td>\n",
       "      <td>-0.168864</td>\n",
       "      <td>-0.053710</td>\n",
       "      <td>-0.159177</td>\n",
       "      <td>-0.193546</td>\n",
       "      <td>-0.218226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342296</td>\n",
       "      <td>-0.993565</td>\n",
       "      <td>-0.594833</td>\n",
       "      <td>1.206153</td>\n",
       "      <td>-0.020178</td>\n",
       "      <td>-0.214950</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>-0.170452</td>\n",
       "      <td>-0.703552</td>\n",
       "      <td>-0.068333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252834</td>\n",
       "      <td>-0.861755</td>\n",
       "      <td>-0.403734</td>\n",
       "      <td>-1.769874</td>\n",
       "      <td>-0.340958</td>\n",
       "      <td>-0.184649</td>\n",
       "      <td>-0.229552</td>\n",
       "      <td>-0.154482</td>\n",
       "      <td>0.672502</td>\n",
       "      <td>-0.592957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.264652</td>\n",
       "      <td>-0.406597</td>\n",
       "      <td>-0.216035</td>\n",
       "      <td>-0.113610</td>\n",
       "      <td>0.076446</td>\n",
       "      <td>-0.156269</td>\n",
       "      <td>0.081681</td>\n",
       "      <td>-0.165843</td>\n",
       "      <td>-0.602908</td>\n",
       "      <td>-0.058965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0_Accounting  x0_Agriculture  x0_Arts  x0_Communications-Media  \\\n",
       "0             0.0             0.0      0.0                      0.0   \n",
       "1             0.0             0.0      0.0                      0.0   \n",
       "2             0.0             0.0      0.0                      1.0   \n",
       "3             0.0             0.0      0.0                      0.0   \n",
       "4             0.0             0.0      0.0                      0.0   \n",
       "..            ...             ...      ...                      ...   \n",
       "63            0.0             0.0      0.0                      0.0   \n",
       "64            0.0             0.0      0.0                      0.0   \n",
       "65            0.0             0.0      0.0                      0.0   \n",
       "66            0.0             0.0      0.0                      0.0   \n",
       "67            0.0             0.0      0.0                      0.0   \n",
       "\n",
       "    x0_Education  x0_Engineering  x0_Fashion  x0_Internet  \\\n",
       "0            0.0             0.0         0.0          0.0   \n",
       "1            0.0             0.0         0.0          0.0   \n",
       "2            0.0             0.0         0.0          0.0   \n",
       "3            0.0             0.0         0.0          0.0   \n",
       "4            0.0             0.0         0.0          0.0   \n",
       "..           ...             ...         ...          ...   \n",
       "63           0.0             0.0         0.0          0.0   \n",
       "64           0.0             0.0         0.0          0.0   \n",
       "65           0.0             0.0         0.0          0.0   \n",
       "66           0.0             0.0         0.0          0.0   \n",
       "67           0.0             0.0         0.0          0.0   \n",
       "\n",
       "    x0_InvestmentBanking  x0_Law  ...  Symbol ratio  Average letters per word  \\\n",
       "0                    0.0     0.0  ...     -0.728211                 -0.373913   \n",
       "1                    0.0     0.0  ...      0.441599                  0.662686   \n",
       "2                    0.0     0.0  ...     -0.215583                  1.093433   \n",
       "3                    0.0     0.0  ...     -0.613411                 -0.303795   \n",
       "4                    1.0     0.0  ...     -0.947118                  1.279772   \n",
       "..                   ...     ...  ...           ...                       ...   \n",
       "63                   0.0     0.0  ...      0.155387                  0.424941   \n",
       "64                   0.0     0.0  ...     -0.791656                  0.483748   \n",
       "65                   0.0     0.0  ...      0.342296                 -0.993565   \n",
       "66                   0.0     0.0  ...     -0.252834                 -0.861755   \n",
       "67                   0.0     0.0  ...     -1.264652                 -0.406597   \n",
       "\n",
       "    Variance of letters per word  Unique words ratio  \\\n",
       "0                      -0.405588           -1.034518   \n",
       "1                      -0.225826            0.630411   \n",
       "2                      -0.397563            1.570226   \n",
       "3                      -0.514383            0.885276   \n",
       "4                      -0.381717            1.409860   \n",
       "..                           ...                 ...   \n",
       "63                     -0.135984            1.149267   \n",
       "64                     -0.272493           -0.283885   \n",
       "65                     -0.594833            1.206153   \n",
       "66                     -0.403734           -1.769874   \n",
       "67                     -0.216035           -0.113610   \n",
       "\n",
       "    Average letters per sentence  Variance of letters per sentence  \\\n",
       "0                      -0.172704                         -0.172048   \n",
       "1                      -0.519892                         -0.226515   \n",
       "2                      -0.562348                         -0.256236   \n",
       "3                      -0.344406                         -0.201200   \n",
       "4                       2.008931                         -0.266308   \n",
       "..                           ...                               ...   \n",
       "63                     -0.463039                         -0.247183   \n",
       "64                     -0.004562                         -0.168864   \n",
       "65                     -0.020178                         -0.214950   \n",
       "66                     -0.340958                         -0.184649   \n",
       "67                      0.076446                         -0.156269   \n",
       "\n",
       "    Average words per sentence  Variance of words per sentence  \\\n",
       "0                    -0.114345                       -0.149941   \n",
       "1                    -0.509414                       -0.208545   \n",
       "2                    -0.608481                       -0.254289   \n",
       "3                    -0.292459                       -0.192371   \n",
       "4                     1.353031                       -0.254289   \n",
       "..                         ...                             ...   \n",
       "63                   -0.499508                       -0.251388   \n",
       "64                   -0.053710                       -0.159177   \n",
       "65                    0.072600                       -0.170452   \n",
       "66                   -0.229552                       -0.154482   \n",
       "67                    0.081681                       -0.165843   \n",
       "\n",
       "    Maximal uppercase ratio per sentence  \\\n",
       "0                              -0.426713   \n",
       "1                               3.848010   \n",
       "2                              -0.491851   \n",
       "3                              -0.337887   \n",
       "4                               0.001620   \n",
       "..                                   ...   \n",
       "63                             -0.330291   \n",
       "64                             -0.193546   \n",
       "65                             -0.703552   \n",
       "66                              0.672502   \n",
       "67                             -0.602908   \n",
       "\n",
       "    Length of the maximal uppercase ratio sentence  \n",
       "0                                        -0.508643  \n",
       "1                                        -0.592957  \n",
       "2                                        -0.068333  \n",
       "3                                        -0.424328  \n",
       "4                                         2.030163  \n",
       "..                                             ...  \n",
       "63                                        0.081559  \n",
       "64                                       -0.218226  \n",
       "65                                       -0.068333  \n",
       "66                                       -0.592957  \n",
       "67                                       -0.058965  \n",
       "\n",
       "[68 rows x 45 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_data = stacking.X_train_numerical_transformed\n",
    "numerical_data_features = np.append(stacking.numerical_transformer.transformers_[0][1].get_feature_names(),\\\n",
    "                     stacking.X_train_numerical.columns.drop(categorial_variables))\n",
    "\n",
    "df_numerical_cluster = pd.DataFrame(numerical_data, columns=numerical_data_features)\n",
    "df_numerical_cluster\n",
    "\n",
    "\n",
    "# Die Features mit den x0 - xi Werten beschreiben die Ausprägungen die kategorialen Variablen\n",
    "# das jeweilige i beschreibt das i-te Element der im Punkt \"Target Variable\" definierten liste categorial_variables\n",
    "# Die verbleibenden Features (ohne xi) sind Standardskaliert, (x - \\mu)/\\sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedModelling:\n",
    "    \n",
    "    def __init__(self,  numerical_model, numerical_model_params,\\\n",
    "                text_model, text_model_params, stacked_transformation_instance,\\\n",
    "                weights=(0.5, 0.5)):\n",
    "        \n",
    "        # Initialize numerical model\n",
    "        self.numerical_model = numerical_model(**numerical_model_params)\n",
    "        \n",
    "        # Initialize text model\n",
    "        self.text_model = text_model(**text_model_params)\n",
    "        \n",
    "        # internalize transformation class\n",
    "        self.stacked_transformation = stacked_transformation_instance\n",
    "        \n",
    "        # model weights\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self):\n",
    "        # train numerical model\n",
    "        self.numerical_model.fit(self.stacked_transformation.X_train_numerical_transformed,\\\n",
    "                             self.stacked_transformation.y_train)\n",
    "                             \n",
    "        print(\"Numerical model finished!\")\n",
    "        \n",
    "        # train textual model\n",
    "        self.text_model.fit(self.stacked_transformation.X_train_text_transformed,\\\n",
    "                             self.stacked_transformation.y_train)\n",
    "                             \n",
    "        print(\"Text model finished!\")\n",
    "              \n",
    "    def predict_text(self, X):\n",
    "        return self.text_model.predict(X)\n",
    "    \n",
    "    def predict_numerical(self, X):\n",
    "        return self.numerical_model.predict(X)\n",
    "\n",
    "    \n",
    "    def optimize_weights(self, X, y):\n",
    "        # get loss for both models\n",
    "        y_pred_numerical = self.numerical_model.predict(X)\n",
    "        y_pred_textual = self.text_model.predict(X)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def weighted_prediction(self, X, weights=None, algo_type=\"classification\"):\n",
    "        if weights == None:\n",
    "            weights = self.weights\n",
    "        \n",
    "        # check if one or more transactions become processed\n",
    "        try:\n",
    "            if type(X) == dict:\n",
    "                X_transformed = self.stacked_transformation.transform_one(X)\n",
    "\n",
    "            else:\n",
    "                X_transformed = self.stacked_transformation.transform_many(X)\n",
    "        except TypeError:\n",
    "            print(\"Check the data type of the input data\")\n",
    "\n",
    "        \n",
    "        return (self.text_model.predict_proba(X_transformed[\"transformed_text\"]),\\\n",
    "                self.numerical_model.predict_proba(X_transformed[\"transformed_numerical\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical model finished!\n",
      "Numerical model finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.79469717, 0.20530286],\n",
       "        [0.03773683, 0.96226317],\n",
       "        [0.9779954 , 0.02200458],\n",
       "        [0.5052613 , 0.49473873],\n",
       "        [0.39564717, 0.60435283],\n",
       "        [0.9688676 , 0.03113239],\n",
       "        [0.05333126, 0.94666874],\n",
       "        [0.9268388 , 0.07316116],\n",
       "        [0.17015117, 0.8298488 ],\n",
       "        [0.05610365, 0.94389635],\n",
       "        [0.6879061 , 0.3120939 ],\n",
       "        [0.0286141 , 0.9713859 ],\n",
       "        [0.08402741, 0.9159726 ],\n",
       "        [0.02014595, 0.97985405],\n",
       "        [0.04644632, 0.9535537 ],\n",
       "        [0.03177005, 0.96822995],\n",
       "        [0.03944409, 0.9605559 ],\n",
       "        [0.10532731, 0.8946727 ],\n",
       "        [0.03247142, 0.9675286 ],\n",
       "        [0.5052613 , 0.49473873],\n",
       "        [0.6597464 , 0.34025362],\n",
       "        [0.6751555 , 0.32484445],\n",
       "        [0.95875484, 0.04124515],\n",
       "        [0.8984462 , 0.10155379],\n",
       "        [0.04871005, 0.95128995],\n",
       "        [0.9465293 , 0.05347068],\n",
       "        [0.5261258 , 0.47387418],\n",
       "        [0.03247142, 0.9675286 ],\n",
       "        [0.94721663, 0.05278336],\n",
       "        [0.03091234, 0.96908766],\n",
       "        [0.02826989, 0.9717301 ],\n",
       "        [0.98878723, 0.01121277],\n",
       "        [0.03826481, 0.9617352 ],\n",
       "        [0.11751592, 0.8824841 ],\n",
       "        [0.8684252 , 0.1315748 ],\n",
       "        [0.24412006, 0.75587994],\n",
       "        [0.02826989, 0.9717301 ],\n",
       "        [0.7192522 , 0.28074777],\n",
       "        [0.01021606, 0.98978394],\n",
       "        [0.5052613 , 0.49473873],\n",
       "        [0.8579438 , 0.14205623],\n",
       "        [0.21268988, 0.7873101 ],\n",
       "        [0.9549512 , 0.04504878],\n",
       "        [0.01405746, 0.98594254],\n",
       "        [0.03621703, 0.96378297],\n",
       "        [0.9908144 , 0.00918561],\n",
       "        [0.04272342, 0.9572766 ],\n",
       "        [0.03128356, 0.96871644],\n",
       "        [0.86303335, 0.13696663],\n",
       "        [0.15366238, 0.8463376 ],\n",
       "        [0.07550836, 0.92449164],\n",
       "        [0.38982683, 0.61017317],\n",
       "        [0.07082725, 0.92917275],\n",
       "        [0.85013556, 0.14986445],\n",
       "        [0.07028699, 0.929713  ],\n",
       "        [0.5052613 , 0.49473873],\n",
       "        [0.79353034, 0.20646966],\n",
       "        [0.00283968, 0.9971603 ],\n",
       "        [0.16528636, 0.83471364],\n",
       "        [0.06557125, 0.93442875],\n",
       "        [0.9539889 , 0.04601109],\n",
       "        [0.95861685, 0.04138315],\n",
       "        [0.02858341, 0.9714166 ],\n",
       "        [0.97452295, 0.02547706],\n",
       "        [0.1315605 , 0.8684395 ],\n",
       "        [0.8068483 , 0.19315171],\n",
       "        [0.8937145 , 0.1062855 ],\n",
       "        [0.8415952 , 0.15840483],\n",
       "        [0.02892995, 0.97107005],\n",
       "        [0.05569732, 0.9443027 ],\n",
       "        [0.1199097 , 0.8800903 ],\n",
       "        [0.16918498, 0.830815  ],\n",
       "        [0.0047664 , 0.9952336 ],\n",
       "        [0.05443376, 0.94556624],\n",
       "        [0.8604722 , 0.13952783],\n",
       "        [0.01041698, 0.989583  ],\n",
       "        [0.18678296, 0.81321704],\n",
       "        [0.8303072 , 0.16969283],\n",
       "        [0.95488966, 0.04511034],\n",
       "        [0.9166479 , 0.08335207],\n",
       "        [0.82721794, 0.17278203],\n",
       "        [0.03169739, 0.9683026 ],\n",
       "        [0.0603146 , 0.9396854 ],\n",
       "        [0.5052613 , 0.49473873],\n",
       "        [0.75887907, 0.2411209 ],\n",
       "        [0.15376377, 0.8462362 ],\n",
       "        [0.9518347 , 0.04816535],\n",
       "        [0.91074777, 0.08925223],\n",
       "        [0.74738383, 0.25261617],\n",
       "        [0.00838125, 0.99161875],\n",
       "        [0.53136706, 0.4686329 ],\n",
       "        [0.5052613 , 0.49473873],\n",
       "        [0.00442851, 0.9955715 ],\n",
       "        [0.9679406 , 0.03205939]], dtype=float32),\n",
       " array([[0.3108114 , 0.6891886 ],\n",
       "        [0.03679538, 0.9632046 ],\n",
       "        [0.9644783 , 0.0355217 ],\n",
       "        [0.04417586, 0.95582414],\n",
       "        [0.11002415, 0.88997585],\n",
       "        [0.95966816, 0.04033183],\n",
       "        [0.13443351, 0.8655665 ],\n",
       "        [0.9013451 , 0.09865493],\n",
       "        [0.03586608, 0.9641339 ],\n",
       "        [0.04286063, 0.9571394 ],\n",
       "        [0.00205404, 0.99794596],\n",
       "        [0.03792036, 0.96207964],\n",
       "        [0.11682558, 0.8831744 ],\n",
       "        [0.05129123, 0.9487088 ],\n",
       "        [0.01582855, 0.98417145],\n",
       "        [0.03535408, 0.9646459 ],\n",
       "        [0.01528406, 0.98471594],\n",
       "        [0.06825536, 0.93174464],\n",
       "        [0.09126461, 0.9087354 ],\n",
       "        [0.9826386 , 0.01736142],\n",
       "        [0.960575  , 0.03942505],\n",
       "        [0.9768321 , 0.02316788],\n",
       "        [0.9518244 , 0.04817556],\n",
       "        [0.9853479 , 0.01465208],\n",
       "        [0.06101644, 0.93898356],\n",
       "        [0.9503026 , 0.0496974 ],\n",
       "        [0.16025072, 0.8397493 ],\n",
       "        [0.02079666, 0.97920334],\n",
       "        [0.7494589 , 0.2505411 ],\n",
       "        [0.01009315, 0.98990685],\n",
       "        [0.01881599, 0.981184  ],\n",
       "        [0.948752  , 0.05124804],\n",
       "        [0.00646001, 0.99354   ],\n",
       "        [0.01025438, 0.9897456 ],\n",
       "        [0.929746  , 0.07025401],\n",
       "        [0.46004176, 0.53995824],\n",
       "        [0.0414933 , 0.9585067 ],\n",
       "        [0.8845221 , 0.11547792],\n",
       "        [0.01538742, 0.9846126 ],\n",
       "        [0.04668075, 0.95331925],\n",
       "        [0.07780331, 0.9221967 ],\n",
       "        [0.08223909, 0.9177609 ],\n",
       "        [0.9916356 , 0.00836437],\n",
       "        [0.0077976 , 0.9922024 ],\n",
       "        [0.01416409, 0.9858359 ],\n",
       "        [0.970401  , 0.02959901],\n",
       "        [0.01231307, 0.98768693],\n",
       "        [0.0478738 , 0.9521262 ],\n",
       "        [0.90297943, 0.09702057],\n",
       "        [0.13250017, 0.8674998 ],\n",
       "        [0.00157404, 0.99842596],\n",
       "        [0.03620219, 0.9637978 ],\n",
       "        [0.01622421, 0.9837758 ],\n",
       "        [0.90157306, 0.09842695],\n",
       "        [0.0037846 , 0.9962154 ],\n",
       "        [0.935912  , 0.06408801],\n",
       "        [0.9442964 , 0.0557036 ],\n",
       "        [0.00286674, 0.99713326],\n",
       "        [0.04584414, 0.95415586],\n",
       "        [0.09232223, 0.90767777],\n",
       "        [0.99047303, 0.00952699],\n",
       "        [0.99059963, 0.00940037],\n",
       "        [0.02464747, 0.9753525 ],\n",
       "        [0.939427  , 0.06057301],\n",
       "        [0.03190845, 0.96809155],\n",
       "        [0.9454404 , 0.05455956],\n",
       "        [0.9558031 , 0.04419689],\n",
       "        [0.50138885, 0.49861115],\n",
       "        [0.01873106, 0.98126894],\n",
       "        [0.01050091, 0.9894991 ],\n",
       "        [0.07528615, 0.92471385],\n",
       "        [0.0030874 , 0.9969126 ],\n",
       "        [0.01504099, 0.984959  ],\n",
       "        [0.02237308, 0.9776269 ],\n",
       "        [0.9875779 , 0.01242209],\n",
       "        [0.03168476, 0.96831524],\n",
       "        [0.02954638, 0.9704536 ],\n",
       "        [0.84747535, 0.15252465],\n",
       "        [0.99484026, 0.00515974],\n",
       "        [0.96788144, 0.03211857],\n",
       "        [0.94784355, 0.05215642],\n",
       "        [0.04508156, 0.95491844],\n",
       "        [0.03103334, 0.96896666],\n",
       "        [0.00741827, 0.9925817 ],\n",
       "        [0.9569402 , 0.04305982],\n",
       "        [0.06414431, 0.9358557 ],\n",
       "        [0.9452718 , 0.05472821],\n",
       "        [0.94006056, 0.05993945],\n",
       "        [0.91769636, 0.08230361],\n",
       "        [0.03549075, 0.96450925],\n",
       "        [0.7075628 , 0.2924372 ],\n",
       "        [0.09238243, 0.90761757],\n",
       "        [0.04158747, 0.9584125 ],\n",
       "        [0.9208456 , 0.0791544 ]], dtype=float32))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_modelling = StackedModelling(XGBClassifier, {}, XGBClassifier, {}, stacking, (0.5, 0.5))\n",
    "stacked_modelling.fit()\n",
    "stacked_modelling.weighted_prediction(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
