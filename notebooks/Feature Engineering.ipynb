{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# universally modules\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# preprocessing and transformation modules\n",
    "import fasttext\n",
    "import Preprocessing\n",
    "from Features import buildFeatures\n",
    "from Modelling import BaggingModelling\n",
    "from Transformation import StackedTransformation\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# model algorithm\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# evaluation modules\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed up the process choose a sample size to randomly draw a sample of the whole daataset\n",
    "sample_size = 1000 \n",
    "\n",
    "# remove all text that contain less than n chars\n",
    "min_chars_per_text = 50\n",
    "\n",
    "# which features will be used for the TF-IDF transformation\n",
    "text_features = \"text_preprocessed\"\n",
    "\n",
    "#### define the target variable and categorial variables used in later transformations ###\n",
    "algo_type = \"regression\" # regression or classification\n",
    "\n",
    "#### Case 1: gender \n",
    "#target_variable = \"gender\"\n",
    "#categorial_variables =  [\"topic\", \"sign\"]\n",
    "\n",
    "# Case 2: topic\n",
    "#target_variable = \"topic\"\n",
    "#categorial_variables =  [\"gender\", \"sign\"]\n",
    "\n",
    "# Case 3: age\n",
    "target_variable = \"age\"\n",
    "categorial_variables =  [\"topic\", \"gender\", \"sign\"]\n",
    "\n",
    "# Case 4: sign\n",
    "#target_variable = \"sign\"\n",
    "#categorial_variables =  [\"gender\", \"topic\"]\n",
    "############################################################################################\n",
    "\n",
    "# use only words that occur at least sqrt_3(X) times \n",
    "min_df_exponent = (1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../resource/data/blogtext.csv\")\n",
    "\n",
    "# draw random sample for faster processing:\n",
    "df = df.sample(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for a mininmal number of letters in a tweet:\n",
    "df = df[df[\"text\"].str.count(r\"[a-zA-Z]\") >= min_chars_per_text]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 907/907 [00:01<00:00, 716.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# append the data\n",
    "features = [buildFeatures(text) for text  in tqdm(df[\"text\"])]\n",
    "\n",
    "# append the data\n",
    "columns = [\"Text length\", \"Number URLs\", \"Number mails\",\\\n",
    "          \"Uppercase ratio\", \"Lowercase ratio\", \"Number ratio\", \"Symbol ratio\",\\\n",
    "          \"Average letters per word\", \"Variance of letters per word\", \"Unique words ratio\",\\\n",
    "          \"Average letters per sentence\", \"Variance of letters per sentence\",\\\n",
    "          \"Average words per sentence\", \"Variance of words per sentence\",\\\n",
    "          \"Maximal uppercase ratio per sentence\", \"Length of the maximal uppercase ratio sentence\"]\n",
    "\n",
    "# merge the features with the original dataset\n",
    "df_preprocessed = df.merge(pd.DataFrame(features, columns=columns), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 907/907 [00:29<00:00, 30.51it/s]\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 907/907 [00:00<00:00, 6724.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# use the preprocessing  module\n",
    "preprocessor = Preprocessing.Preprocessing()\n",
    "df_preprocessed[\"text_preprocessed\"] = preprocessor.ProcessMany(df_preprocessed[\"text\"])\n",
    "\n",
    "# predict the main language\n",
    "model = fasttext.load_model('../src/data/lid.176.ftz')\n",
    "df_preprocessed[\"main_language\"] = [model.predict(text)[0][0].split(\"__\")[-1] for text in tqdm(df_preprocessed[\"text_preprocessed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecassary features\n",
    "df_filtered = df_preprocessed[(df_preprocessed[\"main_language\"] == \"en\")]\\\n",
    "                .drop([\"id\",\"text\",\"date\",\"main_language\"], axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df_filtered.drop(target_variable, axis=1),df_filtered[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the  text transformer class to create two transformers for the textual and the numerical model\n",
    "text_transformer = TfidfVectorizer(ngram_range=(1,1), min_df=int(len(X)**(min_df_exponent)))\n",
    "numerical_transformer = make_column_transformer((OneHotEncoder(handle_unknown=\"ignore\"), categorial_variables)\\\n",
    "                                                       , remainder=StandardScaler())\n",
    "\n",
    "stacking = StackedTransformation(X, y, numerical_transformer, text_transformer, text_features)\n",
    "stacking.build_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_data_features = stacking.text_transformer.get_feature_names()\n",
    "text_data = stacking.X_train_text_transformed.toarray()\n",
    "\n",
    "df_text_cluster = pd.DataFrame(text_data, columns=text_data_features)\n",
    "\n",
    "# Die Features beschreiben die Worte im Text\n",
    "# Die Werte sind die TF*IDF-transformierten Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    numerical_data = stacking.X_train_numerical_transformed.toarray()\n",
    "except:\n",
    "    numerical_data = stacking.X_train_numerical_transformed\n",
    "\n",
    "numerical_data_features = np.append(stacking.numerical_transformer.transformers_[0][1].get_feature_names(),\\\n",
    "                     stacking.X_train_numerical.columns.drop(categorial_variables))\n",
    "\n",
    "df_numerical_cluster = pd.DataFrame(numerical_data, columns=numerical_data_features)\n",
    "#df_numerical_cluster\n",
    "\n",
    "\n",
    "# Die Features mit den x0 - xi Werten beschreiben die Ausprägungen die kategorialen Variablen\n",
    "# das jeweilige i beschreibt das i-te Element der im Punkt \"Target Variable\" definierten liste categorial_variables\n",
    "# Die verbleibenden Features (ohne xi) sind Standardskaliert, (x - \\mu)/\\sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical model finished!\n",
      "Text model finished!\n",
      "Weights have been optimized:\n",
      "                Textual model weight: 0.4812742269935398\n",
      "                Numerical model weight: 0.5187257730064602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Absolute loss textual model              1095.045250\n",
       "Absolute loss numerical model            1015.983943\n",
       "Absolute loss equally weighted model      982.133860\n",
       "Absolute loss optimized weights model     982.133860\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_modelling = BaggingModelling(XGBRegressor, {}, XGBRegressor, {}, stacking, (0.5, 0.5))\n",
    "X_test = stacked_modelling.stacked_transformation.X_test\n",
    "y_test = stacked_modelling.stacked_transformation.y_test\n",
    "\n",
    "\n",
    "stacked_modelling.fit()\n",
    "stacked_modelling.create_report(X_test,y_test, algo_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
